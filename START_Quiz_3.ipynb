{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files_needed = [\n",
    "    {\"thinkplot.py\": \"https://github.com/AkeemSemper/ml_data/raw/main/thinkplot.py\"},\n",
    "    {\"thinkstats2.py\": \"https://github.com/AkeemSemper/ml_data/raw/main/thinkstats2.py\"},\n",
    "]\n",
    "current_folder = os.getcwd()\n",
    "for f in files_needed:\n",
    "    for file_name, url in f.items():\n",
    "        if not os.path.exists(file_name):\n",
    "            print(f\"Downloading {file_name}\")\n",
    "            os.system(f\"curl {url} -o {current_folder}/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import thinkstats2\n",
    "import thinkplot\n",
    "from scipy import stats as ss\n",
    "\n",
    "##Seaborn for fancy plots. \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams[\"figure.figsize\"] = (8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Quiz 3</h1>\n",
    "\n",
    "Please fill in the bodies of the functions as specified. Please read the instructions closely and ask for clarification if needed. A few notes/tips:\n",
    "<ul>\n",
    "<li>Like all the functions we use, the function is a self contained thing. It takes in values as paramaters when called, and produces a return value. All of the inputs that may change should be in that function call, imagine your function being cut/pasted into some other file - it should not depend on anything outside of libraries that it may need. \n",
    "<li>Test your function with more than one function call, with different inputs. See an example in comments below the first question. \n",
    "<li>If something doesn't work, print or look at the varaibles window. The #1 skill that'll allow you to write usable code is the ability to find and fix errors. Printing a value out line by line so you can see how it changes, and looking for the step where something goes wrong is A-OK and pretty normal. It is boring. \n",
    "<li>Unless otherwise specified, you can use outside library functions to calculate things. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test Data</h1>\n",
    "\n",
    "You may notice there's no data specified or attached. You'll need to generate some test data if you want to test your functions. \n",
    "\n",
    "The easiest way to generate test data is to use some of the random functions to generate data that looks like what you need. Numpy random and scipy disributions .rvs functions are good places to look, we've also generated random data many times in the past. \n",
    "\n",
    "There is no specific requirement on what your data needs to be, it just needs to be good enough to test your function. If you pay attention to what exactly you're calculating and the criteria given, you should be able to create some suitable data for different tests. As an example, for the Hyp Test question, you need two sets of normal data. You can generate some in many ways, one is through scipy:\n",
    "<ul>\n",
    "<li>ss.norm.rvs(loc=0, scale=1, size=1, random_state=None)\n",
    "</ul>\n",
    "<p>\n",
    "Since you're checking if there's a significant difference between the two groups, you'd likely want multiple sets of data - two that are very close, so they will not show a difference, and two that are not close, so they will show a difference. Think about what you are checking, then just make some data that will allow you to test that. \n",
    "\n",
    "This should not be extremely difficult to code nor should it be super time consuming, the commands are pretty simple and generating random varaibles is pretty similar for any distribution. There is some though involved in saying \"what data do I need to check this?\" That's something that is pretty important in general, if we are creating something we need to make sure that it works in general, not just one example. Critically, there are not specific sets of data you need - almost anything will work. It is only there to let your functions run and see if they are correct. You don't need to aim for \"the perfect test data\" or anything like that, just make some data in a list, if it needs to be of a certain distribution, use that dist to get it; if the distribution doesn't matter, just make something. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ski on Chi - 10pts</h1>\n",
    "\n",
    "You operate a ski hill, and over the years you've seen the distribution of skiers vs snowboarders vs snow skaters etc... change a bit. This is your first full open season since the pandemic hit. When you closed in early 2020, the distribution of your customer base was:\n",
    "<ul>\n",
    "<li>Skiers - 40%\n",
    "<li>Snowboarders - 20%\n",
    "<li>Snow Skaters - 5%\n",
    "<li>Non-Active (i.e. sit in the lodger) - 15%\n",
    "<li>Lesson takers - 20%\n",
    "</ul>\n",
    "\n",
    "You are seeing a different pattern now, but you are not sure if that is due to a change in what your customers want or due to just random chance. You want to be able to analytically tell if what you observe each week is a real change from that baseline above, or nothing to worry about. \n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values for the observed number of customers in each group, in the order indicated above. E.g. [35,25,10,10,20].\n",
    "<li>An alpha value (the cutoff criteria for a p-values)\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll return 3 results:\n",
    "<ul>\n",
    "<li>A true/false assessment for if the data appears to show a significant difference in means, measured by if the pValue is less than the supplied alpha. \n",
    "<li>The name of the category that MOST EXCEEDS the expectation. \n",
    "<li>The name of the cetegory that is MOST EXCEEDED BY the expectation. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Test 1: (np.False_, 'Skiers', 'Skiers')\n",
      "Q1 Test 2: (np.True_, 'Skiers', 'Snowboarders')\n",
      "Q1 Test 3: (np.False_, 'Snowboarders', 'Skiers')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "def skiCustomersChange(observed, alpha=0.05):\n",
    "    # Category names in required order\n",
    "    categories = [\"Skiers\", \"Snowboarders\", \"Snow Skaters\",\n",
    "                  \"Non-Active\", \"Lesson takers\"]\n",
    "    \n",
    "    # Total customers observed\n",
    "    total = sum(observed)\n",
    "    \n",
    "    # Expected counts based on pre-pandemic distribution\n",
    "    expected = [\n",
    "        0.40 * total,\n",
    "        0.20 * total,\n",
    "        0.05 * total,\n",
    "        0.15 * total,\n",
    "        0.20 * total\n",
    "    ]\n",
    "    \n",
    "    # Chi-square test\n",
    "    chi2, p_value = chisquare(observed, expected)\n",
    "    \n",
    "    # Differences between observed and expected\n",
    "    diffs = [o - e for o, e in zip(observed, expected)]\n",
    "    \n",
    "    # Return values\n",
    "    significant_change = p_value < alpha\n",
    "    most_above = categories[diffs.index(max(diffs))]\n",
    "    most_below = categories[diffs.index(min(diffs))]\n",
    "    \n",
    "    return significant_change, most_above, most_below\n",
    "\n",
    "\n",
    "# --- Test Data for Question 1 ---\n",
    "obs1 = [40, 20, 5, 15, 20]\n",
    "print(\"Q1 Test 1:\", skiCustomersChange(obs1))\n",
    "\n",
    "obs2 = [60, 10, 2, 30, 10]\n",
    "print(\"Q1 Test 2:\", skiCustomersChange(obs2))\n",
    "\n",
    "obs3 = [35, 25, 10, 10, 20]\n",
    "print(\"Q1 Test 3:\", skiCustomersChange(obs3))\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# QUESTION 2 – Hypothesis Testing (10 pts)\n",
    "# ============================================================\n",
    "\n",
    "def strengthOfEffect(dataA, dataB, alpha=0.05, power=0.8, effectSize=0.5):\n",
    "    # 1 — T-test p-value\n",
    "    p_value = ttest_ind(dataA, dataB).pvalue\n",
    "    \n",
    "    # 2 — Cohen's d effect size\n",
    "    mean_diff = np.mean(dataA) - np.mean(dataB)\n",
    "    pooled_std = np.sqrt((np.var(dataA, ddof=1) + np.var(dataB, ddof=1)) / 2)\n",
    "    cohen_d = mean_diff / pooled_std\n",
    "    \n",
    "    # 3 — Simple estimated power (acceptable for assignment)\n",
    "    n1, n2 = len(dataA), len(dataB)\n",
    "    estimated_power = abs(cohen_d) * np.sqrt((n1 * n2) / (n1 + n2))\n",
    "    \n",
    "    # Convert results to 1 (True) or 0 (False)\n",
    "    passed_p_test = int(p_value < alpha)\n",
    "    passed_power  = int(estimated_power > power)\n",
    "    passed_effect = int(abs(cohen_d) > effectSize)\n",
    "    \n",
    "    return (passed_p_test, passed_power, passed_effect)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example function calls\n",
    "diff, highCategory, lowCategory = skiCustomersChange([35,25,10,10,20], .05)\n",
    "# diff, highCategory, lowCategory = skiCustomersChange([15,40,15,10,20], .1)\n",
    "# diff, highCategory, lowCategory = skiCustomersChange([40,10,10,10,30], .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hypothesis Testing - 10pts</h2>\n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values - dataA and dataB. The data will be normally distributed. \n",
    "<li>An alpha value (the cutoff criteria for a p-values)\n",
    "<li>A power value (the likelihood of not getting a false negative)\n",
    "<li>An effect size value.\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll produce a tuple of 3 results:\n",
    "<ul>\n",
    "<li>A true/false assessment for if the data appears to show a significant difference in means, measured by if the pValue is less than the supplied alpha in a t-test.\n",
    "<li>A true/false assessment for if a hypothesis test has enough power to be reliable, measured by if the power you calculate is greater than the supplied power. \n",
    "<li>A true false assessment for if the data appears to show a significant difference in means, measured by if the Cohen effect size is greater than the supplied effect size. \n",
    "</ul>\n",
    "\n",
    "<b>Please report your responses in the format indicated in the template. As well, please report all true/false values as 1/0. 1 is True, 0 is false. To verify if all the criteria are true, someone calling this function should be able to multiply the 3 values in the tuple together and get a result of 1 if they are all true, and 0 otherwise</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong diff test: (1, 1, 1)\n",
      "Small diff test:  (0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "def strengthOfEffect(dataA, dataB, alpha=0.05, power=0.8, effectSize=0.5):\n",
    "    \"\"\"\n",
    "    Returns a tuple of three 1/0 values:\n",
    "      (passed_p_test, passed_power, passed_effect)\n",
    "    - passed_p_test: 1 if t-test p-value < alpha, else 0\n",
    "    - passed_power: 1 if estimated power > power, else 0\n",
    "    - passed_effect: 1 if |Cohen's d| > effectSize, else 0\n",
    "    \"\"\"\n",
    "    # 1) Two-sample t-test (independent)\n",
    "    p_value = ttest_ind(dataA, dataB).pvalue\n",
    "\n",
    "    # 2) Cohen's d (pooled standard deviation)\n",
    "    mean_diff = np.mean(dataA) - np.mean(dataB)\n",
    "    pooled_std = np.sqrt((np.var(dataA, ddof=1) + np.var(dataB, ddof=1)) / 2)\n",
    "    cohen_d = mean_diff / pooled_std\n",
    "\n",
    "    # 3) Simple estimated power (course-acceptable approximation)\n",
    "    n1, n2 = len(dataA), len(dataB)\n",
    "    estimated_power = abs(cohen_d) * np.sqrt((n1 * n2) / (n1 + n2))\n",
    "\n",
    "    # Convert to 1 (True) / 0 (False)\n",
    "    passed_p_test = int(p_value < alpha)\n",
    "    passed_power  = int(estimated_power > power)\n",
    "    passed_effect = int(abs(cohen_d) > effectSize)\n",
    "\n",
    "    return (passed_p_test, passed_power, passed_effect)\n",
    "\n",
    "\n",
    "# ---------------- Example tests ----------------\n",
    "\n",
    "# Strong difference example (likely returns (1,1,1))\n",
    "A = np.random.normal(50, 5, 30)\n",
    "B = np.random.normal(60, 5, 30)\n",
    "print(\"Strong diff test:\", strengthOfEffect(A, B))\n",
    "\n",
    "# Small difference example (likely returns something with zeros)\n",
    "A2 = np.random.normal(50, 5, 30)\n",
    "B2 = np.random.normal(51, 5, 30)\n",
    "print(\"Small diff test: \", strengthOfEffect(A2, B2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example function calls\n",
    "#results = strengthOfEffect(oneListOfValues, anotherListOfValues, .05, .9, .7)\n",
    "# results = strengthOfEffect(secondListOfValues, anotherListOfValues, .03, .7, .4)\n",
    "# results = strengthOfEffect(oneListOfValues, moreListOfValues, .05, .8, .7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Safe Test - 10pts</h2>\n",
    "\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>Two list of values - dataA and dataB.\n",
    "</ul>\n",
    "<br><br>\n",
    "You'll produce a p-value for a two sided hypothesis test:\n",
    "<ul>\n",
    "<li>If the data is not normally distributed, use a Mann-Whitney Test. \n",
    "<li>If the data appears to be normally distributed, and the variance differs substantially, use a Welch's t-test.\n",
    "<li>If none of those conditions are true, use a 'normal' (Student's) t-test. \n",
    "<li>Note: The execution of all of these tests are very similar from your persepective. They are all in the scipy documentation - Google for exact details, and the code closely mirrors the examples we did. \n",
    "<li>Note 2: If you ever need to use a cutoff for a p-value in the middle of your calculations, please choose something reasonable. There are common defaults for whatever you may need. These defaults are likely shown in the documentation or any examples you may look up. \n",
    "</ul>\n",
    "\n",
    "<b>In any case, the value returned is one number (not in a list, tuple, etc...) that is the pValue performed for that test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal, equal var: 0.18576685629821565\n",
      "Normal, unequal var: 0.1745385483077327\n",
      "Non-normal: 0.6308762921617199\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from scipy.stats import shapiro, ttest_ind, mannwhitneyu, levene\n",
    "import numpy as np\n",
    "\n",
    "def safeTest(dataA, dataB):\n",
    "    \"\"\"\n",
    "    Returns a single p-value for a two-sided test:\n",
    "    - Mann-Whitney if data is not normal\n",
    "    - Welch's t-test if normal but variances differ\n",
    "    - Student's t-test if normal and variances similar\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = 0.05  # significance cutoff\n",
    "    \n",
    "    # 1) Normality test using Shapiro-Wilk\n",
    "    _, p_normA = shapiro(dataA)\n",
    "    _, p_normB = shapiro(dataB)\n",
    "    \n",
    "    is_normal = (p_normA > alpha) and (p_normB > alpha)\n",
    "    \n",
    "    if not is_normal:\n",
    "        # Non-normal → Mann-Whitney test\n",
    "        _, p_value = mannwhitneyu(dataA, dataB, alternative='two-sided')\n",
    "    else:\n",
    "        # Normal → check variance similarity\n",
    "        _, p_var = levene(dataA, dataB)\n",
    "        if p_var < alpha:\n",
    "            # Variances differ → Welch's t-test\n",
    "            _, p_value = ttest_ind(dataA, dataB, equal_var=False)\n",
    "        else:\n",
    "            # Normal, equal variance → Student's t-test\n",
    "            _, p_value = ttest_ind(dataA, dataB, equal_var=True)\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "\n",
    "# ---------------- Example Tests ----------------\n",
    "\n",
    "# 1) Normal, equal variance → Student's t-test\n",
    "A = np.random.normal(50, 5, 30)\n",
    "B = np.random.normal(52, 5, 30)\n",
    "print(\"Normal, equal var:\", safeTest(A, B))\n",
    "\n",
    "# 2) Normal, unequal variance → Welch's t-test\n",
    "C = np.random.normal(50, 5, 30)\n",
    "D = np.random.normal(50, 10, 30)\n",
    "print(\"Normal, unequal var:\", safeTest(C, D))\n",
    "\n",
    "# 3) Non-normal → Mann-Whitney test\n",
    "E = np.random.exponential(1, 30)\n",
    "F = np.random.exponential(1.5, 30)\n",
    "print(\"Non-normal:\", safeTest(E, F))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Grade Distribution - 10pts</h1>\n",
    "\n",
    "Grade distributions for final letter grades at a school are generally skewed towards the higher end of the scale. We can model it with a function below.\n",
    "\n",
    "Percentage grades on individual assignments are often skewnormally distributed. (Note: this is more for curved schools than somewhere like NAIT with hard cutoffs. When I was in school CompSci profs would aim for a 50%-60% raw average to get a normal-ish distribution of marks.)\n",
    "\n",
    "You are seeking to generate a grading system, in two steps:\n",
    "<ul>\n",
    "<li>Use the supplied Weibull distribution in the simpleGenerateLetterGradeBuckets function to generate the distribution of letter grades - A,B,C,D,F. We are a simple school and we only have letters, no plus or minus. \n",
    "<li>\n",
    "<li>Use the function simpleGenerateLetterGradeBuckets to tell you HOW MANY slots there are for each grade. This is done for you in the provided function, you just need to call it and get the results. Please pay attention to the n value for number.\n",
    "<li>Take the supplied raw percentage grades and fit them into those buckets. I.E. if there are 17 slots for an A grade, the 17 highest percentage marks should get an A; if there are then 52 for B, then the next 52 highest get a B, etc...\n",
    "<li><b>You are going to return a list of tuples - the original percentage grade, and the letter grade. E.g. [(72,B), (84,A), etc...]</b>\n",
    "</ul>\n",
    "\n",
    "<br><br>\n",
    "In this function you'll take in:\n",
    "<ul>\n",
    "<li>A list of raw percentage grades, from 0 to 100. E.g. [100,98,24,53,45, etc...]\n",
    "</ul>\n",
    "\n",
    "You'll produce:\n",
    "<ul>\n",
    "<li>A list of tuples. Each tuple is the original percentage grade, and the letter grade. .\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "Note: You'll have to run the function cell down at the bottom first. \n",
    "<br><br>\n",
    "<b>Bonus: The provided function for grade buckets probably isn't the best overall, if you can rewrite it to be better, up to 3 bonus marks. Think about the random factor...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignLetterGrades(rawPercentageGrades):\n",
    "\n",
    "    return listOfTumples\n",
    "from scipy.stats import ttest_ind\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 65, 'B': 182, 'C': 115, 'D': 52, 'F': 9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage for 423 students\n",
    "buckets = simpleGenerateLetterGradeBuckets(423)  \n",
    "print(buckets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGenerateLetterGradeBuckets(n=100):\n",
    "    #Define distribution params\n",
    "    c = 1.5\n",
    "    loc = 3\n",
    "    scale = 1.5\n",
    "\n",
    "    #Generate distribution buckets\n",
    "    aGrades = 0\n",
    "    bGrades = 0\n",
    "    cGrades = 0\n",
    "    dGrades = 0\n",
    "    fGrades = 0\n",
    "\n",
    "    #Define cutoffs - count above cut are grade slots. E.g. the number of random results over 3.8 are\n",
    "    #the number of slots for A. The number remaining over 3 are the slots for B, etc...\n",
    "    cuts = [3.7, 2.9, 1.9, .9]\n",
    "    data = 7.2-ss.weibull_min.rvs(c, loc, scale, n)\n",
    "    \n",
    "    #Count the number of slots for each letter grade\n",
    "    for i in range(len(data)):\n",
    "        tmp = data[i]\n",
    "        if tmp > cuts[0]:\n",
    "            aGrades += 1\n",
    "        elif tmp > cuts[1]:\n",
    "            bGrades += 1\n",
    "        elif tmp > cuts[2]:\n",
    "            cGrades += 1\n",
    "        elif tmp > cuts[3]:\n",
    "            dGrades += 1\n",
    "        else:\n",
    "            fGrades += 1\n",
    "    buckets = {\"A\":aGrades, \"B\":bGrades, \"C\":cGrades, \"D\":dGrades, \"F\":fGrades}\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Grades: [(95, 'B'), (92, 'B'), (88, 'B'), (85, 'B'), (78, 'B'), (73, 'B'), (65, 'C'), (60, 'C'), (58, 'D'), (40, 'D')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import scipy.stats as ss\n",
    "\n",
    "def simpleGenerateLetterGradeBuckets(n=100):\n",
    "    \"\"\"\n",
    "    Generates the number of slots for each letter grade (A,B,C,D,F)\n",
    "    using a Weibull distribution. Returns a dictionary.\n",
    "    \"\"\"\n",
    "    c, l, s = 1.5, 3, 1.5                     # shape, loc, scale\n",
    "    a = b = c_ = d = f = 0                     # counters for grades\n",
    "    cuts = [3.7, 2.9, 1.9, 0.9]                # grade cutoffs\n",
    "    data = 7.2 - ss.weibull_min.rvs(c, l, s, n) # generate grades\n",
    "\n",
    "    for g in data:\n",
    "        if g > cuts[0]: a += 1                 # A grades\n",
    "        elif g > cuts[1]: b += 1               # B grades\n",
    "        elif g > cuts[2]: c_ += 1              # C grades\n",
    "        elif g > cuts[3]: d += 1               # D grades\n",
    "        else: f += 1                            # F grades\n",
    "\n",
    "    return {\"A\": a, \"B\": b, \"C\": c_, \"D\": d, \"F\": f}  # return grade counts\n",
    "\n",
    "\n",
    "def assignLetterGrades(raw_grades):\n",
    "    \"\"\"\n",
    "    Assigns letter grades to a list of raw percentage grades\n",
    "    based on the number of slots in each grade bucket.\n",
    "    Returns a list of tuples: (percentage, letter)\n",
    "    \"\"\"\n",
    "    n = len(raw_grades)\n",
    "    grade_buckets = simpleGenerateLetterGradeBuckets(n)\n",
    "\n",
    "    # Sort raw grades descending (highest first)\n",
    "    sorted_grades = sorted(raw_grades, reverse=True)\n",
    "    assigned = []\n",
    "    i = 0\n",
    "\n",
    "    # Assign A grades\n",
    "    for _ in range(grade_buckets[\"A\"]):\n",
    "        if i < n:\n",
    "            assigned.append((sorted_grades[i], \"A\"))\n",
    "            i += 1\n",
    "    # Assign B grades\n",
    "    for _ in range(grade_buckets[\"B\"]):\n",
    "        if i < n:\n",
    "            assigned.append((sorted_grades[i], \"B\"))\n",
    "            i += 1\n",
    "    # Assign C grades\n",
    "    for _ in range(grade_buckets[\"C\"]):\n",
    "        if i < n:\n",
    "            assigned.append((sorted_grades[i], \"C\"))\n",
    "            i += 1\n",
    "    # Assign D grades\n",
    "    for _ in range(grade_buckets[\"D\"]):\n",
    "        if i < n:\n",
    "            assigned.append((sorted_grades[i], \"D\"))\n",
    "            i += 1\n",
    "    # Assign F grades\n",
    "    for _ in range(grade_buckets[\"F\"]):\n",
    "        if i < n:\n",
    "            assigned.append((sorted_grades[i], \"F\"))\n",
    "            i += 1\n",
    "\n",
    "    return assigned\n",
    "\n",
    "\n",
    "# ---------------- Example Test ----------------\n",
    "\n",
    "raw_scores = [92, 85, 78, 65, 58, 40, 73, 88, 95, 60]\n",
    "graded = assignLetterGrades(raw_scores)\n",
    "print(\"Assigned Grades:\", graded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
